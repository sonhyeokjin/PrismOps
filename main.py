# main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from litellm import completion
import time
from dotenv import load_dotenv

# 1. FastAPI ì•± ìƒì„±
app = FastAPI(
    title="PrismOps Gateway",
    description="LLM A/B Testing & Routing Gateway",
    version="0.1.0"
)


# 2. ë°ì´í„° ëª¨ë¸ ì •ì˜ (ìš”ì²­/ì‘ë‹µ í˜•ì‹ì„ ë¯¸ë¦¬ ì•½ì†í•¨)
class ChatRequest(BaseModel):
    model: str = "ollama/gemma:7b"  # ê¸°ë³¸ê°’ ì„¤ì •
    message: str


class ChatResponse(BaseModel):
    reply: str
    model_used: str
    latency: float


# 3. í—¬ìŠ¤ ì²´í¬ìš© ì—”ë“œí¬ì¸íŠ¸ (ì„œë²„ ì‚´ì•„ìˆë‹ˆ?)
@app.get("/")
async def health_check():
    return {"status": "ok", "service": "PrismOps Gateway"}


# 4. ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (í•µì‹¬ ê¸°ëŠ¥!)
@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    start_time = time.time()

    print(f"ğŸ“¥ ìš”ì²­ ìˆ˜ì‹ : {request.model} / ë‚´ìš©: {request.message}")

    try:
        # LiteLLMìœ¼ë¡œ AIì—ê²Œ ì§ˆë¬¸ ë˜ì§€ê¸°
        response = completion(
            model=request.model,
            messages=[{"role": "user", "content": request.message}],
            api_base="http://localhost:11434"
        )

        # ë‹µë³€ ì¶”ì¶œ
        reply_text = response.choices[0].message.content
        end_time = time.time()
        process_time = round(end_time - start_time, 2)

        return ChatResponse(
            reply=reply_text,
            model_used=request.model,
            latency=process_time
        )

    except Exception as e:
        # ì—ëŸ¬ ë‚˜ë©´ 500 ì—ëŸ¬ ë°˜í™˜
        raise HTTPException(status_code=500, detail=str(e))