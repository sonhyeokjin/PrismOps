import time
import random
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from litellm import completion
from dotenv import load_dotenv

# Prometheus ëª¨ë‹ˆí„°ë§ ë„êµ¬
from prometheus_fastapi_instrumentator import Instrumentator

# ëª¨ë“ˆ
import config
import logger

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="PrismOps Gateway",
    description="A/B Testing Router for LLMs",
    version="0.3.1"  # ë²„ê·¸ ìˆ˜ì • ë²„ì „ ì—…
)

# Prometheus ê³„ì¸¡ê¸° ì‹¤í–‰ (/metrics ì—”ë“œí¬ì¸íŠ¸ ë…¸ì¶œ)
Instrumentator().instrument(app).expose(app)


# ë°ì´í„° ëª¨ë¸ ì •ì˜
class ChatRequest(BaseModel):
    message: str


class ChatResponse(BaseModel):
    reply: str
    model: str
    latency: float


@app.get("/")
async def health_check():
    return {"status": "ok", "mode": "A/B Routing"}


@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest, background_tasks: BackgroundTasks):
    start_time = time.time()

    # 1. ë¼ìš°íŒ… ê²°ì • (50:50 í™•ë¥ )
    if random.random() < config.ROUTING_RATIO:
        selected_model = config.MODEL_B  # Cloud (OpenAI)
        tag = "Cloud(B)"
    else:
        selected_model = config.MODEL_A  # Local (Ollama)
        tag = "Local(A)"

    print(f"ğŸ”€ [Router] {tag} ì„ íƒë¨ -> {selected_model}")

    # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ API ì£¼ì†Œ ë¶„ê¸° ì²˜ë¦¬
    # ê¸°ë³¸ê°’ì€ Noneìœ¼ë¡œ ì„¤ì • (OpenAIëŠ” ì£¼ì†Œë¥¼ ë”°ë¡œ ì„¤ì •í•  í•„ìš” ì—†ìŒ)
    custom_api_base = None

    # ë§Œì•½ ë¡œì»¬ ëª¨ë¸(Ollama)ì´ ì„ íƒë˜ì—ˆë‹¤ë©´, ë„ì»¤ í˜¸ìŠ¤íŠ¸ ì£¼ì†Œ(host.docker.internal)ë¥¼ ì‚¬ìš©
    if selected_model == config.MODEL_A:
        custom_api_base = config.OLLAMA_API_BASE

    try:
        # 2. ëª¨ë¸ í˜¸ì¶œ
        response = completion(
            model=selected_model,
            messages=[{"role": "user", "content": request.message}],
            api_base=custom_api_base
        )

        reply_text = response.choices[0].message.content

        # 3. ì‹œê°„ ì¸¡ì •
        end_time = time.time()
        latency = round(end_time - start_time, 2)

        # 4. ë¹„ë™ê¸° ë¡œê·¸ ì €ì¥
        log_data = {
            "user_message": request.message,
            "reply_snippet": reply_text[:30] + "...",
            "model": selected_model,
            "latency": latency,
            "status": "success"
        }
        background_tasks.add_task(logger.log_transaction, log_data)

        return ChatResponse(
            reply=reply_text,
            model=selected_model,
            latency=latency
        )

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ë¡œê·¸
        error_data = {
            "user_message": request.message,
            "model": selected_model,
            "error": str(e),
            "status": "failed"
        }
        background_tasks.add_task(logger.log_transaction, error_data)

        print(f"[Error] {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))